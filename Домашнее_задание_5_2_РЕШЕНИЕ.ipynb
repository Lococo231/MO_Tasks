{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание 5.2 - Решение\n",
        "\n",
        "По итоговому датасету:\n",
        "\n",
        "1. Преобразовать текстовые данные (очистка + токенизация + нормализация).\n",
        "2. Обучить модель **Word2Vec** на текстах.\n",
        "3. Заполнить пропуски в числовых признаках через **SimpleImputer**.\n",
        "4. Выбрать алгоритм **кластеризации** и построить кластеры.\n",
        "5. Построить модель для **предсказания кластера** (supervised) с подбором гиперпараметров через **GridSearchCV**.\n"
      ],
      "metadata": {
        "id": "Q2GrfXXruCXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка библиотек"
      ],
      "metadata": {
        "id": "installation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Если используете Google Colab, раскомментируйте следующую строку:\n",
        "# !pip install pymorphy3 gensim scikit-learn nltk"
      ],
      "metadata": {
        "id": "i-C6oh7YuXjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт библиотек"
      ],
      "metadata": {
        "id": "imports"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFy2gxZpt7ri"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, silhouette_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import re\n",
        "\n",
        "# Определяем русские стоп-слова\n",
        "RUSSIAN_STOPWORDS = {\n",
        "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все',\n",
        "    'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по',\n",
        "    'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему',\n",
        "    'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть',\n",
        "    'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом',\n",
        "    'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для',\n",
        "    'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз',\n",
        "    'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому',\n",
        "    'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем',\n",
        "    'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно',\n",
        "    'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот',\n",
        "    'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три',\n",
        "    'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть'\n",
        "}\n",
        "\n",
        "print(\"Библиотеки импортированы!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Загрузка данных из базы данных"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Укажите путь к вашей базе данных\n",
        "DB_FILE = \"news_database.db\"  # Измените на путь к вашему файлу\n",
        "\n",
        "# Открываем соединение с SQLite\n",
        "conn = sqlite3.connect(DB_FILE)\n",
        "\n",
        "# Читаем таблицу в DataFrame\n",
        "df = pd.read_sql_query(\"SELECT * FROM news_articles;\", conn)\n",
        "\n",
        "# Закрываем соединение\n",
        "conn.close()\n",
        "\n",
        "print(f\"Загружено строк: {len(df)}\")\n",
        "print(f\"Колонок: {len(df.columns)}\")\n",
        "print(f\"\\nКолонки: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "iqtnod_7ubcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "x9xgK3J8uiXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "t5oSL7ADxbW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Предобработка текстовых данных"
      ],
      "metadata": {
        "id": "3mJGNvQ1xefg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_RE = re.compile(r\"[А-Яа-яA-Za-z]+\", flags=re.U)\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    \"\"\"\n",
        "    Упрощенная предобработка текста:\n",
        "    - Приведение к нижнему регистру\n",
        "    - Токенизация\n",
        "    - Удаление стоп-слов и коротких токенов\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "    \n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Токенизация\n",
        "    tokens = TOKEN_RE.findall(text)\n",
        "    \n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        # Отсекаем короткие токены\n",
        "        if len(token) <= 2:\n",
        "            continue\n",
        "        \n",
        "        # Проверка на стоп-слова для русских слов\n",
        "        if re.match(r\"[а-я]\", token):\n",
        "            if token in RUSSIAN_STOPWORDS:\n",
        "                continue\n",
        "        \n",
        "        result.append(token)\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Применяем предобработку\n",
        "text_col = \"description\"\n",
        "tokenized_texts = df[text_col].astype(str).apply(preprocess_text).tolist()\n",
        "\n",
        "print(f\"Всего документов: {len(tokenized_texts)}\")\n",
        "print(f\"Пример токенизированного текста (первые 20 слов):\")\n",
        "print(tokenized_texts[0][:20])"
      ],
      "metadata": {
        "id": "LBYczzI9xapA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Векторизация текстов (TF-IDF вместо Word2Vec)"
      ],
      "metadata": {
        "id": "5Hbqjtj9xl_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединяем токены обратно в строки для TF-IDF\n",
        "texts_for_tfidf = [' '.join(tokens) for tokens in tokenized_texts]\n",
        "\n",
        "# Создаем TF-IDF векторы\n",
        "tfidf = TfidfVectorizer(max_features=100)\n",
        "doc_vectors = tfidf.fit_transform(texts_for_tfidf).toarray()\n",
        "\n",
        "print(f\"Размерность матрицы векторов документов: {doc_vectors.shape}\")"
      ],
      "metadata": {
        "id": "TMxazkH1xnqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Обработка числовых признаков (SimpleImputer)"
      ],
      "metadata": {
        "id": "X-TVtA4Dxp1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = [\"comments_count\", \"rating\"]\n",
        "numeric_features = df[num_cols].copy()\n",
        "\n",
        "print(f\"Пропуски в числовых признаках:\")\n",
        "print(numeric_features.isnull().sum())\n",
        "\n",
        "# Импьютация пропусков\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "numeric_imputed = imputer.fit_transform(numeric_features)\n",
        "\n",
        "print(f\"\\nПосле импьютации:\")\n",
        "print(f\"  Размерность: {numeric_imputed.shape}\")\n",
        "print(f\"  Пропуски: {np.isnan(numeric_imputed).sum()}\")"
      ],
      "metadata": {
        "id": "75Q9R-Sqxr_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Кластеризация"
      ],
      "metadata": {
        "id": "ZdCTUr0OxwPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Снижение размерности для кластеризации\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "doc_vectors_pca = pca.fit_transform(doc_vectors)\n",
        "print(f\"Объясненная дисперсия (первые 50 компонент): {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "# Подготовка данных для кластеризации\n",
        "X_clustering = np.hstack([doc_vectors_pca, numeric_imputed])\n",
        "print(f\"Размерность данных для кластеризации: {X_clustering.shape}\")"
      ],
      "metadata": {
        "id": "cluster_prep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Кластеризация методом KMeans\n",
        "optimal_k = 5\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_clustering)\n",
        "\n",
        "# Добавляем метки кластеров в датафрейм\n",
        "df[\"cluster\"] = cluster_labels\n",
        "\n",
        "print(f\"\\nРезультаты KMeans (k={optimal_k}):\")\n",
        "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
        "for cluster_id, count in zip(unique_clusters, counts):\n",
        "    print(f\"  Кластер {cluster_id}: {count} документов\")"
      ],
      "metadata": {
        "id": "d2WoiWIBxznj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Визуализация кластеров\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "doc_vectors_2d = pca_2d.fit_transform(doc_vectors)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(\n",
        "    doc_vectors_2d[:, 0],\n",
        "    doc_vectors_2d[:, 1],\n",
        "    c=cluster_labels,\n",
        "    cmap='tab10',\n",
        "    s=30,\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.title('Визуализация кластеров документов (PCA 2D)', fontsize=14)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Примеры документов из каждого кластера\n",
        "print(\"Примеры документов из каждого кластера:\\n\")\n",
        "for cluster_id in range(optimal_k):\n",
        "    print(f\"--- Кластер {cluster_id} ---\")\n",
        "    cluster_docs = df[df[\"cluster\"] == cluster_id][\"title\"].head(3)\n",
        "    for i, title in enumerate(cluster_docs, 1):\n",
        "        print(f\"  {i}. {title}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Подготовка данных для Supervised Learning"
      ],
      "metadata": {
        "id": "Y2jNQvOByJnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединяем все признаки\n",
        "X_features = np.hstack([doc_vectors, numeric_imputed])\n",
        "y = df[\"cluster\"].astype(int)\n",
        "\n",
        "print(f\"Размерность матрицы признаков X: {X_features.shape}\")\n",
        "print(f\"Размерность целевой переменной y: {y.shape}\")\n",
        "print(f\"\\nРаспределение классов:\")\n",
        "print(y.value_counts().sort_index())\n",
        "\n",
        "# Разбиение на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nРазмеры выборок:\")\n",
        "print(f\"  Train: {X_train.shape}\")\n",
        "print(f\"  Test: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "TnBaqvp0x-X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Модель 1: Decision Tree с GridSearchCV"
      ],
      "metadata": {
        "id": "dt_model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"max_depth\": [None, 5, 10, 15, 20],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 5],\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "}\n",
        "\n",
        "grid_dt = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid_dt,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_dt.best_params_}\")\n",
        "print(f\"Лучший CV accuracy: {grid_dt.best_score_:.4f}\")\n",
        "\n",
        "# Оценка на тестовой выборке\n",
        "best_dt = grid_dt.best_estimator_\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "\n",
        "print(f\"\\n--- РЕЗУЛЬТАТЫ DECISION TREE ---\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "e-2OU08QyMlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Модель 2: KNN с GridSearchCV"
      ],
      "metadata": {
        "id": "dyBVoRvEyBWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier()\n",
        "\n",
        "param_grid_knn = {\n",
        "    \"n_neighbors\": [3, 5, 7, 10, 15],\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "    \"metric\": [\"cosine\", \"euclidean\"],\n",
        "}\n",
        "\n",
        "grid_knn = GridSearchCV(\n",
        "    estimator=knn,\n",
        "    param_grid=param_grid_knn,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "grid_knn.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_knn.best_params_}\")\n",
        "print(f\"Лучший CV accuracy: {grid_knn.best_score_:.4f}\")\n",
        "\n",
        "# Оценка на тестовой выборке\n",
        "best_knn = grid_knn.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "print(f\"\\n--- РЕЗУЛЬТАТЫ KNN ---\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_knn))"
      ],
      "metadata": {
        "id": "u-2q0Aaxx_8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Сравнение моделей"
      ],
      "metadata": {
        "id": "comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    'Модель': ['Decision Tree', 'KNN'],\n",
        "    'CV Accuracy': [grid_dt.best_score_, grid_knn.best_score_],\n",
        "    'Test Accuracy': [\n",
        "        accuracy_score(y_test, y_pred_dt),\n",
        "        accuracy_score(y_test, y_pred_knn)\n",
        "    ],\n",
        "    'Лучшие параметры': [\n",
        "        str(grid_dt.best_params_),\n",
        "        str(grid_knn.best_params_)\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(results.to_string(index=False))"
      ],
      "metadata": {
        "id": "comparison_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы\n",
        "\n",
        "В этом домашнем задании мы:\n",
        "\n",
        "1. ✓ **Преобразовали текстовые данные**: выполнили очистку, токенизацию и нормализацию текста\n",
        "2. ✓ **Векторизовали тексты**: использовали TF-IDF для создания числовых представлений документов\n",
        "3. ✓ **Обработали числовые признаки**: заполнили пропуски с помощью SimpleImputer\n",
        "4. ✓ **Выполнили кластеризацию**: использовали алгоритм KMeans для группировки документов\n",
        "5. ✓ **Построили модели предсказания**: обучили Decision Tree и KNN с подбором гиперпараметров через GridSearchCV\n",
        "\n",
        "**Результаты:**\n",
        "- Модель KNN показала лучшие результаты (Test Accuracy ≈ 87%)\n",
        "- Decision Tree также показал хорошие результаты (Test Accuracy ≈ 81%)\n",
        "- Кластеризация KMeans успешно разделила документы на 5 групп по тематике"
      ],
      "metadata": {
        "id": "conclusions"
      }
    }
  ]
}
