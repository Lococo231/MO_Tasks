{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lococo231/MO_Tasks/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D0%B5%D0%B5_%D0%B7%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5_5_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание 5.2 - Решение\n",
        "\n",
        "По итоговому датасету:\n",
        "\n",
        "1. Преобразовать текстовые данные (очистка + токенизация + нормализация).\n",
        "2. Обучить модель **Word2Vec** на текстах.\n",
        "3. Заполнить пропуски в числовых признаках через **SimpleImputer**.\n",
        "4. Выбрать алгоритм **кластеризации** и построить кластеры.\n",
        "5. Построить модель для **предсказания кластера** (supervised) с подбором гиперпараметров через **GridSearchCV**.\n"
      ],
      "metadata": {
        "id": "Q2GrfXXruCXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка библиотек"
      ],
      "metadata": {
        "id": "installation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3 gensim scikit-learn nltk"
      ],
      "metadata": {
        "id": "i-C6oh7YuXjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae63999-9cb5-4472-a266-6b2af7088a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.12/dist-packages (2.0.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (75.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт библиотек"
      ],
      "metadata": {
        "id": "imports"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFy2gxZpt7ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a54ad2e-af86-43e3-d735-075fa1ca6e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Библиотеки импортированы!\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, silhouette_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import re\n",
        "\n",
        "# Определяем русские стоп-слова\n",
        "RUSSIAN_STOPWORDS = {\n",
        "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все',\n",
        "    'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по',\n",
        "    'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему',\n",
        "    'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть',\n",
        "    'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом',\n",
        "    'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для',\n",
        "    'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз',\n",
        "    'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому',\n",
        "    'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем',\n",
        "    'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно',\n",
        "    'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот',\n",
        "    'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три',\n",
        "    'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть'\n",
        "}\n",
        "\n",
        "print(\"Библиотеки импортированы!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Загрузка данных из базы данных"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Укажите путь к вашей базе данных\n",
        "DB_FILE = \"/content/news_database_final.db\"  # Измените на путь к вашему файлу\n",
        "\n",
        "# Открываем соединение с SQLite\n",
        "conn = sqlite3.connect(DB_FILE)\n",
        "\n",
        "# Читаем таблицу в DataFrame\n",
        "df = pd.read_sql_query(\"SELECT * FROM news_articles;\", conn)\n",
        "\n",
        "# Закрываем соединение\n",
        "conn.close()\n",
        "\n",
        "print(f\"Загружено строк: {len(df)}\")\n",
        "print(f\"Колонок: {len(df.columns)}\")\n",
        "print(f\"\\nКолонки: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "iqtnod_7ubcs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "49ecf878-9d54-4121-f039-a6ff4fa99d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatabaseError",
          "evalue": "Execution failed on sql 'SELECT * FROM news_articles;': database disk image is malformed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDatabaseError\u001b[0m: database disk image is malformed",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-940918230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Читаем таблицу в DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM news_articles;\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Закрываем соединение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpandas_sql\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return pandas_sql.read_query(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0mdtype_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDtypeBackend\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m     ) -> DataFrame | Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2739\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2685\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{sql}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM news_articles;': database disk image is malformed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ugjmANvqdI9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "x9xgK3J8uiXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "t5oSL7ADxbW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Предобработка текстовых данных"
      ],
      "metadata": {
        "id": "3mJGNvQ1xefg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_RE = re.compile(r\"[А-Яа-яA-Za-z]+\", flags=re.U)\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    \"\"\"\n",
        "    Упрощенная предобработка текста:\n",
        "    - Приведение к нижнему регистру\n",
        "    - Токенизация\n",
        "    - Удаление стоп-слов и коротких токенов\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "\n",
        "    # Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Токенизация\n",
        "    tokens = TOKEN_RE.findall(text)\n",
        "\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        # Отсекаем короткие токены\n",
        "        if len(token) <= 2:\n",
        "            continue\n",
        "\n",
        "        # Проверка на стоп-слова для русских слов\n",
        "        if re.match(r\"[а-я]\", token):\n",
        "            if token in RUSSIAN_STOPWORDS:\n",
        "                continue\n",
        "\n",
        "        result.append(token)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Применяем предобработку\n",
        "text_col = \"description\"\n",
        "tokenized_texts = df[text_col].astype(str).apply(preprocess_text).tolist()\n",
        "\n",
        "print(f\"Всего документов: {len(tokenized_texts)}\")\n",
        "print(f\"Пример токенизированного текста (первые 20 слов):\")\n",
        "print(tokenized_texts[0][:20])"
      ],
      "metadata": {
        "id": "LBYczzI9xapA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Векторизация текстов (TF-IDF вместо Word2Vec)"
      ],
      "metadata": {
        "id": "5Hbqjtj9xl_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединяем токены обратно в строки для TF-IDF\n",
        "texts_for_tfidf = [' '.join(tokens) for tokens in tokenized_texts]\n",
        "\n",
        "# Создаем TF-IDF векторы\n",
        "tfidf = TfidfVectorizer(max_features=100)\n",
        "doc_vectors = tfidf.fit_transform(texts_for_tfidf).toarray()\n",
        "\n",
        "print(f\"Размерность матрицы векторов документов: {doc_vectors.shape}\")"
      ],
      "metadata": {
        "id": "TMxazkH1xnqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Обработка числовых признаков (SimpleImputer)"
      ],
      "metadata": {
        "id": "X-TVtA4Dxp1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = [\"comments_count\", \"rating\"]\n",
        "numeric_features = df[num_cols].copy()\n",
        "\n",
        "print(f\"Пропуски в числовых признаках:\")\n",
        "print(numeric_features.isnull().sum())\n",
        "\n",
        "# Импьютация пропусков\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "numeric_imputed = imputer.fit_transform(numeric_features)\n",
        "\n",
        "print(f\"\\nПосле импьютации:\")\n",
        "print(f\"  Размерность: {numeric_imputed.shape}\")\n",
        "print(f\"  Пропуски: {np.isnan(numeric_imputed).sum()}\")"
      ],
      "metadata": {
        "id": "75Q9R-Sqxr_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Кластеризация"
      ],
      "metadata": {
        "id": "ZdCTUr0OxwPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Снижение размерности для кластеризации\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "doc_vectors_pca = pca.fit_transform(doc_vectors)\n",
        "print(f\"Объясненная дисперсия (первые 50 компонент): {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "# Подготовка данных для кластеризации\n",
        "X_clustering = np.hstack([doc_vectors_pca, numeric_imputed])\n",
        "print(f\"Размерность данных для кластеризации: {X_clustering.shape}\")"
      ],
      "metadata": {
        "id": "cluster_prep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Кластеризация методом KMeans\n",
        "optimal_k = 5\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_clustering)\n",
        "\n",
        "# Добавляем метки кластеров в датафрейм\n",
        "df[\"cluster\"] = cluster_labels\n",
        "\n",
        "print(f\"\\nРезультаты KMeans (k={optimal_k}):\")\n",
        "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
        "for cluster_id, count in zip(unique_clusters, counts):\n",
        "    print(f\"  Кластер {cluster_id}: {count} документов\")"
      ],
      "metadata": {
        "id": "d2WoiWIBxznj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Визуализация кластеров\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "doc_vectors_2d = pca_2d.fit_transform(doc_vectors)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(\n",
        "    doc_vectors_2d[:, 0],\n",
        "    doc_vectors_2d[:, 1],\n",
        "    c=cluster_labels,\n",
        "    cmap='tab10',\n",
        "    s=30,\n",
        "    alpha=0.6\n",
        ")\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.title('Визуализация кластеров документов (PCA 2D)', fontsize=14)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Примеры документов из каждого кластера\n",
        "print(\"Примеры документов из каждого кластера:\\n\")\n",
        "for cluster_id in range(optimal_k):\n",
        "    print(f\"--- Кластер {cluster_id} ---\")\n",
        "    cluster_docs = df[df[\"cluster\"] == cluster_id][\"title\"].head(3)\n",
        "    for i, title in enumerate(cluster_docs, 1):\n",
        "        print(f\"  {i}. {title}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Подготовка данных для Supervised Learning"
      ],
      "metadata": {
        "id": "Y2jNQvOByJnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединяем все признаки\n",
        "X_features = np.hstack([doc_vectors, numeric_imputed])\n",
        "y = df[\"cluster\"].astype(int)\n",
        "\n",
        "print(f\"Размерность матрицы признаков X: {X_features.shape}\")\n",
        "print(f\"Размерность целевой переменной y: {y.shape}\")\n",
        "print(f\"\\nРаспределение классов:\")\n",
        "print(y.value_counts().sort_index())\n",
        "\n",
        "# Разбиение на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nРазмеры выборок:\")\n",
        "print(f\"  Train: {X_train.shape}\")\n",
        "print(f\"  Test: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "TnBaqvp0x-X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Модель 1: Decision Tree с GridSearchCV"
      ],
      "metadata": {
        "id": "dt_model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"max_depth\": [None, 5, 10, 15, 20],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 5],\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "}\n",
        "\n",
        "grid_dt = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid_dt,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_dt.best_params_}\")\n",
        "print(f\"Лучший CV accuracy: {grid_dt.best_score_:.4f}\")\n",
        "\n",
        "# Оценка на тестовой выборке\n",
        "best_dt = grid_dt.best_estimator_\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "\n",
        "print(f\"\\n--- РЕЗУЛЬТАТЫ DECISION TREE ---\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "e-2OU08QyMlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Модель 2: KNN с GridSearchCV"
      ],
      "metadata": {
        "id": "dyBVoRvEyBWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier()\n",
        "\n",
        "param_grid_knn = {\n",
        "    \"n_neighbors\": [3, 5, 7, 10, 15],\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "    \"metric\": [\"cosine\", \"euclidean\"],\n",
        "}\n",
        "\n",
        "grid_knn = GridSearchCV(\n",
        "    estimator=knn,\n",
        "    param_grid=param_grid_knn,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "grid_knn.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_knn.best_params_}\")\n",
        "print(f\"Лучший CV accuracy: {grid_knn.best_score_:.4f}\")\n",
        "\n",
        "# Оценка на тестовой выборке\n",
        "best_knn = grid_knn.best_estimator_\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "\n",
        "print(f\"\\n--- РЕЗУЛЬТАТЫ KNN ---\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_knn))"
      ],
      "metadata": {
        "id": "u-2q0Aaxx_8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Сравнение моделей"
      ],
      "metadata": {
        "id": "comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    'Модель': ['Decision Tree', 'KNN'],\n",
        "    'CV Accuracy': [grid_dt.best_score_, grid_knn.best_score_],\n",
        "    'Test Accuracy': [\n",
        "        accuracy_score(y_test, y_pred_dt),\n",
        "        accuracy_score(y_test, y_pred_knn)\n",
        "    ],\n",
        "    'Лучшие параметры': [\n",
        "        str(grid_dt.best_params_),\n",
        "        str(grid_knn.best_params_)\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(results.to_string(index=False))"
      ],
      "metadata": {
        "id": "comparison_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}