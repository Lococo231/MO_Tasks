{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PljxH4Pt3ZNQ"
      },
      "source": [
        "# ML –ü—Ä–æ–µ–∫—Ç: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
        "\n",
        "## –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏\n",
        "\n",
        "**–¶–µ–ª—å**: –°–æ–∑–¥–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤.\n",
        "\n",
        "**–ó–∞–¥–∞—á–∏**:\n",
        "1. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ –∏–∑ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è —Ç–µ–≥–æ–≤ (—Ü–µ–Ω—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–ª–æ–≤ = —Ç–µ–≥–∏)\n",
        "2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–∞–º–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º\n",
        "3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –Ω–æ–≤–æ—Å—Ç—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "4. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º\n",
        "\n",
        "**–ú–µ—Ç—Ä–∏–∫–∏**:\n",
        "- –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: Silhouette Score, Davies-Bouldin Index\n",
        "- –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: Accuracy, F1-score, Precision, Recall\n",
        "- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: Confusion Matrix –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YR5fJ7Uv3ln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQEba5xe3ZNR"
      },
      "source": [
        "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inhMEJhD3ZNR"
      },
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "!pip install -q pymorphy3 gensim nltk wordcloud scikit-learn pandas numpy matplotlib seaborn --break-system-packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSiHPxYl3ZNS"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import json\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Preprocessing\n",
        "import pymorphy3\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# ML\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print(\"‚úì –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9KQO0Ck3ZNS"
      },
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
        "RUSSIAN_STOPWORDS = {\n",
        "    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ',\n",
        "    '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ',\n",
        "    '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É',\n",
        "    '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å',\n",
        "    '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º',\n",
        "    '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è',\n",
        "    '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑',\n",
        "    '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É',\n",
        "    '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º',\n",
        "    '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ',\n",
        "    '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç',\n",
        "    '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏',\n",
        "    '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å',\n",
        "    '–≥–æ–¥', '–≥–æ–¥–∞', '–ª–µ—Ç', '—Ç–∞–∫–∂–µ', '–±–æ–ª–µ–µ', '—ç—Ç–æ', '–¥–µ–Ω—å', '—Å–≤–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '–Ω–æ–≤—ã–π',\n",
        "    '–≤–µ—Å—å', '–≤—Ä–µ–º—è', '–º–µ—Å—Ç–æ', '—á–∏—Å–ª–æ', '–¥–∞—Ç—å', '—Å–∫–∞–∑–∞—Ç—å', '–º–æ—á—å'\n",
        "}\n",
        "\n",
        "print(\"‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ1lBiBu3ZNT"
      },
      "source": [
        "## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8z982BR3ZNT"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite\n",
        "db_path = '/content/drive/MyDrive/Colab Notebooks/news_database_final.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "df_sql = pd.read_sql_query(\"SELECT * FROM news_articles;\", conn)\n",
        "conn.close()\n",
        "\n",
        "print(f\"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ SQLite: {len(df_sql)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "print(f\"–ö–æ–ª–æ–Ω–∫–∏: {df_sql.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhfrCVlp3ZNU"
      },
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSONL (–µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
        "jsonl_files = list(Path('/content/drive/MyDrive/gazeta_train.jsonl').glob('*.jsonl'))\n",
        "\n",
        "df_jsonl_list = []\n",
        "if jsonl_files:\n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ JSONL —Ñ–∞–π–ª–æ–≤: {len(jsonl_files)}\")\n",
        "    for jsonl_file in jsonl_files:\n",
        "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    if 'root' in data:\n",
        "                        data = data['root']\n",
        "                    df_jsonl_list.append(data)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    if df_jsonl_list:\n",
        "        df_jsonl = pd.DataFrame(df_jsonl_list)\n",
        "        print(f\"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ JSONL: {len(df_jsonl)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        print(f\"–ö–æ–ª–æ–Ω–∫–∏: {df_jsonl.columns.tolist()}\")\n",
        "else:\n",
        "    df_jsonl = pd.DataFrame()\n",
        "    print(\"JSONL —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL5egD8R3ZNU"
      },
      "outputs": [],
      "source": [
        "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
        "if not df_jsonl.empty:\n",
        "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –æ–±—â–µ–º—É —Ñ–æ—Ä–º–∞—Ç—É\n",
        "    df_jsonl = df_jsonl.rename(columns={'text': 'description', 'summary': 'summary_text'})\n",
        "\n",
        "    # –í—ã–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    common_cols = ['title', 'description', 'url']\n",
        "\n",
        "    df_sql_subset = df_sql[common_cols].copy()\n",
        "    df_jsonl_subset = df_jsonl[[col for col in common_cols if col in df_jsonl.columns]].copy()\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
        "    df = pd.concat([df_sql_subset, df_jsonl_subset], ignore_index=True)\n",
        "    df = df.drop_duplicates(subset=['url'], keep='first').reset_index(drop=True)\n",
        "    print(f\"‚úì –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π\")\n",
        "else:\n",
        "    df = df_sql[['guid', 'title', 'description', 'url', 'published_at', 'source_name']].copy()\n",
        "    print(f\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ SQLite –¥–∞–Ω–Ω—ã–µ: {len(df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "print(f\"\\n–ò—Ç–æ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk8vsWgo3ZNV"
      },
      "source": [
        "## 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7alCHO2-3ZNW"
      },
      "outputs": [],
      "source": [
        "# –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
        "print(\"=\" * 60)\n",
        "print(\"–ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df.columns)}\")\n",
        "print(f\"\\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(f\"\\n–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0])\n",
        "\n",
        "print(f\"\\n–î—É–±–ª–∏–∫–∞—Ç—ã: {df.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCc9NfO63ZNW"
      },
      "outputs": [],
      "source": [
        "# –ü–µ—Ä–≤–∏—á–Ω—ã–π –æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt-SyFMj3ZNW"
      },
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤\n",
        "df['text_length'] = df['description'].fillna('').str.len()\n",
        "df['title_length'] = df['title'].fillna('').str.len()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–ò–ù–´ –¢–ï–ö–°–¢–û–í\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n–û–ø–∏—Å–∞–Ω–∏–µ (description):\")\n",
        "print(df['text_length'].describe())\n",
        "print(\"\\n–ó–∞–≥–æ–ª–æ–≤–æ–∫ (title):\")\n",
        "print(df['title_length'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOkInXBY3ZNW"
      },
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ–ø–∏—Å–∞–Ω–∏–π', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)')\n",
        "axes[0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
        "axes[0].axvline(df['text_length'].mean(), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {df[\"text_length\"].mean():.0f}')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(df['title_length'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)')\n",
        "axes[1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
        "axes[1].axvline(df['title_length'].mean(), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {df[\"title_length\"].mean():.0f}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 1: –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {df['text_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤, \"\n",
        "      f\"—á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UpW1cg23ZNX"
      },
      "outputs": [],
      "source": [
        "# –£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø—É—Å—Ç—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º\n",
        "df_clean = df[df['text_length'] > 50].copy()\n",
        "print(f\"–£–¥–∞–ª–µ–Ω–æ {len(df) - len(df_clean)} –∑–∞–ø–∏—Å–µ–π —Å –∫–æ—Ä–æ—Ç–∫–∏–º/–ø—É—Å—Ç—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º\")\n",
        "print(f\"–û—Å—Ç–∞–ª–æ—Å—å: {len(df_clean)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
        "\n",
        "df = df_clean.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVELogpf3ZNX"
      },
      "source": [
        "## 4. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKzynony3ZNX"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, lemmatize=True):\n",
        "    \"\"\"\n",
        "    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞:\n",
        "    - –û—á–∏—Å—Ç–∫–∞ –æ—Ç HTML\n",
        "    - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "    - –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text) if text is not None else \"\"\n",
        "\n",
        "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "    text = text.lower()\n",
        "\n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã)\n",
        "    tokens = re.findall(r'[–∞-—è—ë]+', text)\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        if len(token) <= 2:  # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–æ–∫–µ–Ω—ã\n",
        "            continue\n",
        "        if token in RUSSIAN_STOPWORDS:  # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
        "            continue\n",
        "\n",
        "        if lemmatize:\n",
        "            # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
        "            parsed = morph.parse(token)[0]\n",
        "            lemma = parsed.normal_form\n",
        "            result.append(lemma)\n",
        "        else:\n",
        "            result.append(token)\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"–§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ—Ç–æ–≤–∞\")\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "sample_text = \"–í —É—Ö–æ–¥—è—â–µ–º –≥–æ–¥—É –∏–Ω—Ñ–ª—è—Ü–∏—è –≤ –†–æ—Å—Å–∏–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–º –º–∏–Ω–∏–º—É–º–µ\"\n",
        "print(f\"\\n–ü—Ä–∏–º–µ—Ä:\\n–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {sample_text}\")\n",
        "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π: {preprocess_text(sample_text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS_xUv1k3ZNX"
      },
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –∏ –æ–ø–∏—Å–∞–Ω–∏—è–º\n",
        "print(\"–ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
        "\n",
        "df['title_tokens'] = df['title'].apply(lambda x: preprocess_text(x, lemmatize=True))\n",
        "df['desc_tokens'] = df['description'].apply(lambda x: preprocess_text(x, lemmatize=True))\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ\n",
        "df['all_tokens'] = df['title_tokens'] + df['desc_tokens']\n",
        "\n",
        "# –î–ª—è TF-IDF –Ω—É–∂–µ–Ω —Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏\n",
        "df['processed_text'] = df['all_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "print(f\"‚úì –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
        "print(f\"–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 20 —Å–ª–æ–≤):\\n{df['all_tokens'].iloc[0][:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EHb0bDl3ZNX"
      },
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞—Ä—è\n",
        "all_words = []\n",
        "for tokens in df['all_tokens']:\n",
        "    all_words.extend(tokens)\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"\\n=\" * 60)\n",
        "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–õ–û–í–ê–†–Ø\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(word_counts)}\")\n",
        "print(f\"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(all_words)}\")\n",
        "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {len(all_words) / len(df):.1f} —Å–ª–æ–≤\")\n",
        "\n",
        "print(f\"\\n–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:\")\n",
        "for word, count in word_counts.most_common(20):\n",
        "    print(f\"  {word}: {count}\")\n",
        "\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 2: –°–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(word_counts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤, \"\n",
        "      f\"—á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ —Ç–µ–º–∞—Ç–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPCCocT23ZNX"
      },
      "outputs": [],
      "source": [
        "# –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "text_for_cloud = ' '.join(all_words)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "wordcloud = WordCloud(width=1600, height=800,\n",
        "                     background_color='white',\n",
        "                     colormap='viridis',\n",
        "                     max_words=100).generate(text_for_cloud)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('–û–±–ª–∞–∫–æ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 3: –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞–Ω–∏–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π, \"\n",
        "      f\"—Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Ç–µ–º–∞—Ç–∏–∫ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkoBGfvy3ZNY"
      },
      "source": [
        "## 5. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ (—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–≥–æ–≤)\n",
        "\n",
        "**–¶–µ–ª—å**: –ù–∞–π—Ç–∏ —Ü–µ–Ω—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–Ω—É—Ç —Ç–µ–≥–∞–º–∏ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exsOjuz13ZNY"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å: —Å–ª–æ–≤–æ -> —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è\n",
        "word_to_docs = defaultdict(list)\n",
        "for doc_id, tokens in enumerate(df['all_tokens']):\n",
        "    for token in set(tokens):  # –∏—Å–ø–æ–ª—å–∑—É–µ–º set —á—Ç–æ–±—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è\n",
        "        word_to_docs[token].append(doc_id)\n",
        "\n",
        "# –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞: –¥–æ–ª–∂–Ω—ã –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∏–Ω–∏–º—É–º –≤ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏ –º–∞–∫—Å–∏–º—É–º –≤ 70% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "min_docs = 3\n",
        "max_docs = int(len(df) * 0.7)\n",
        "\n",
        "filtered_words = [word for word, docs in word_to_docs.items()\n",
        "                 if min_docs <= len(docs) <= max_docs]\n",
        "\n",
        "print(f\"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {len(filtered_words)}\")\n",
        "print(f\"(–≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ {min_docs}-{max_docs} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X24kLDhU3ZNY"
      },
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å–ª–æ–≤: –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –¥–ª—è —Å–ª–æ–≤\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º \"–¥–æ–∫—É–º–µ–Ω—Ç—ã\" –¥–ª—è —Å–ª–æ–≤ (–∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ = –¥–æ–∫—É–º–µ–Ω—Ç)\n",
        "word_documents = []\n",
        "word_list = []\n",
        "\n",
        "for word in filtered_words:\n",
        "    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —Å–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–ª–æ–≤, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –≤ —Ç–µ—Ö –∂–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
        "    doc_ids = word_to_docs[word]\n",
        "    context_words = []\n",
        "\n",
        "    for doc_id in doc_ids:\n",
        "        context_words.extend(df['all_tokens'].iloc[doc_id])\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º —Å–∞–º–æ —Å–ª–æ–≤–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
        "    context_words = [w for w in context_words if w != word]\n",
        "\n",
        "    word_documents.append(' '.join(context_words))\n",
        "    word_list.append(word)\n",
        "\n",
        "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(word_documents)} '–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤' –¥–ª—è —Å–ª–æ–≤\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyUbAoIE3ZNY"
      },
      "outputs": [],
      "source": [
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é TF-IDF\n",
        "word_vectorizer = TfidfVectorizer(max_features=200, min_df=2, max_df=0.8)\n",
        "word_vectors = word_vectorizer.fit_transform(word_documents)\n",
        "\n",
        "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã —Å–ª–æ–≤: {word_vectors.shape}\")\n",
        "print(f\"({word_vectors.shape[0]} —Å–ª–æ–≤ √ó {word_vectors.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9VLVkOh3ZNY",
        "outputId": "5c6852cb-5ac9-48ee-f440-36e4f5cba953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å–ª–æ–≤...\n",
            "K=5: Inertia=10480.86, Silhouette=0.031\n",
            "K=10: Inertia=9701.99, Silhouette=0.038\n",
            "K=15: Inertia=9250.36, Silhouette=0.030\n",
            "K=20: Inertia=8914.24, Silhouette=0.030\n",
            "K=25: Inertia=8662.76, Silhouette=0.020\n"
          ]
        }
      ],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –ª–æ–∫—Ç—è\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "K_range = range(5, 31, 5)\n",
        "\n",
        "print(\"–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å–ª–æ–≤...\")\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(word_vectors)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(word_vectors, kmeans.labels_))\n",
        "    print(f\"K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(K_range, inertias, 'bo-')\n",
        "axes[0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')\n",
        "axes[0].set_ylabel('Inertia')\n",
        "axes[0].set_title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
        "axes[1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')\n",
        "axes[1].set_ylabel('Silhouette Score')\n",
        "axes[1].set_title('Silhouette Score –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "optimal_k_words = K_range[np.argmax(silhouette_scores)]\n",
        "print(f\"\\n‚úì –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–ª–æ–≤: {optimal_k_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPNLbx0T3ZNY"
      },
      "outputs": [],
      "source": [
        "# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º K\n",
        "kmeans_words = KMeans(n_clusters=optimal_k_words, random_state=42, n_init=10)\n",
        "word_clusters = kmeans_words.fit_predict(word_vectors)\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤\n",
        "sil_score_words = silhouette_score(word_vectors, word_clusters)\n",
        "db_score_words = davies_bouldin_score(word_vectors.toarray(), word_clusters)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò –°–õ–û–í\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_k_words}\")\n",
        "print(f\"Silhouette Score: {sil_score_words:.3f}\")\n",
        "print(f\"Davies-Bouldin Index: {db_score_words:.3f}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Å–ª–æ–≤\n",
        "word_cluster_df = pd.DataFrame({\n",
        "    'word': word_list,\n",
        "    'cluster': word_clusters\n",
        "})\n",
        "\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:\")\n",
        "print(word_cluster_df['cluster'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZsWOYVm3ZNZ"
      },
      "outputs": [],
      "source": [
        "# –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–≥–∏: –Ω–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
        "tags_dict = {}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–¢–ï–ì–ò (–¶–ï–ù–¢–†–ê–õ–¨–ù–´–ï –°–õ–û–í–ê –ö–õ–ê–°–¢–ï–†–û–í)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for cluster_id in range(optimal_k_words):\n",
        "    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "    cluster_words = word_cluster_df[word_cluster_df['cluster'] == cluster_id]['word'].tolist()\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
        "    cluster_indices = [i for i, w in enumerate(word_list) if w in cluster_words]\n",
        "\n",
        "    if not cluster_indices:\n",
        "        continue\n",
        "\n",
        "    # –¶–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "    cluster_center = kmeans_words.cluster_centers_[cluster_id]\n",
        "\n",
        "    # –ù–∞—Ö–æ–¥–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –æ—Ç —Å–ª–æ–≤ –¥–æ —Ü–µ–Ω—Ç—Ä–∞\n",
        "    distances = []\n",
        "    for idx in cluster_indices:\n",
        "        dist = np.linalg.norm(word_vectors[idx].toarray() - cluster_center)\n",
        "        distances.append((word_list[idx], dist))\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –∏ –±–µ—Ä–µ–º 5 —Å–∞–º—ã—Ö –±–ª–∏–∑–∫–∏—Ö\n",
        "    closest_words = sorted(distances, key=lambda x: x[1])[:5]\n",
        "    tags = [word for word, _ in closest_words]\n",
        "\n",
        "    tags_dict[cluster_id] = tags\n",
        "\n",
        "    print(f\"\\n–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {len(cluster_words)} —Å–ª–æ–≤\")\n",
        "    print(f\"  –¢–µ–≥–∏: {', '.join(tags)}\")\n",
        "\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 4: –°–æ–∑–¥–∞–Ω–æ {len(tags_dict)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (—Ç–µ–≥–æ–≤) \"\n",
        "      f\"–¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe76Iv7s3ZNZ"
      },
      "source": [
        "## 6. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCCuaTTt3ZNZ"
      },
      "outputs": [],
      "source": [
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é TF-IDF\n",
        "doc_vectorizer = TfidfVectorizer(max_features=300, min_df=2, max_df=0.8)\n",
        "doc_vectors = doc_vectorizer.fit_transform(df['processed_text'])\n",
        "\n",
        "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_vectors.shape}\")\n",
        "print(f\"({doc_vectors.shape[0]} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ √ó {doc_vectors.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15jvjbrl3ZNZ"
      },
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "inertias_docs = []\n",
        "silhouette_scores_docs = []\n",
        "K_range_docs = range(3, 16, 2)\n",
        "\n",
        "print(\"–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π...\")\n",
        "\n",
        "for k in K_range_docs:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(doc_vectors)\n",
        "    inertias_docs.append(kmeans.inertia_)\n",
        "    silhouette_scores_docs.append(silhouette_score(doc_vectors, kmeans.labels_))\n",
        "    print(f\"K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores_docs[-1]:.3f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(K_range_docs, inertias_docs, 'bo-')\n",
        "axes[0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')\n",
        "axes[0].set_ylabel('Inertia')\n",
        "axes[0].set_title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(K_range_docs, silhouette_scores_docs, 'ro-')\n",
        "axes[1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')\n",
        "axes[1].set_ylabel('Silhouette Score')\n",
        "axes[1].set_title('Silhouette Score –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "optimal_k_docs = K_range_docs[np.argmax(silhouette_scores_docs)]\n",
        "print(f\"\\n‚úì –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π: {optimal_k_docs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3fhyBIc3ZNa"
      },
      "outputs": [],
      "source": [
        "# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º K\n",
        "kmeans_docs = KMeans(n_clusters=optimal_k_docs, random_state=42, n_init=10)\n",
        "doc_clusters = kmeans_docs.fit_predict(doc_vectors)\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ dataframe\n",
        "df['cluster'] = doc_clusters\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
        "sil_score_docs = silhouette_score(doc_vectors, doc_clusters)\n",
        "db_score_docs = davies_bouldin_score(doc_vectors.toarray(), doc_clusters)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò –ù–û–í–û–°–¢–ï–ô\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_k_docs}\")\n",
        "print(f\"Silhouette Score: {sil_score_docs:.3f}\")\n",
        "print(f\"Davies-Bouldin Index: {db_score_docs:.3f}\")\n",
        "\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:\")\n",
        "cluster_distribution = df['cluster'].value_counts().sort_index()\n",
        "for cluster_id, count in cluster_distribution.items():\n",
        "    print(f\"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –Ω–æ–≤–æ—Å—Ç–µ–π ({count/len(df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSS57IvV3ZNa"
      },
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é PCA\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "doc_vectors_2d = pca.fit_transform(doc_vectors.toarray())\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "scatter = plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1],\n",
        "                     c=doc_clusters, cmap='tab10', s=50, alpha=0.6, edgecolors='black')\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.title('–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π (PCA)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% –¥–∏—Å–ø–µ—Ä—Å–∏–∏)')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% –¥–∏—Å–ø–µ—Ä—Å–∏–∏)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 5: –ù–æ–≤–æ—Å—Ç–∏ —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {optimal_k_docs} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞. \"\n",
        "      f\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á–µ—Ç–∫—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–∞–º.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQB3iSc93ZNa"
      },
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: —Ç–æ–ø-—Å–ª–æ–≤–∞ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
        "print(\"=\" * 60)\n",
        "print(\"–•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ê –ö–õ–ê–°–¢–ï–†–û–í –ù–û–í–û–°–¢–ï–ô\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "feature_names = doc_vectorizer.get_feature_names_out()\n",
        "\n",
        "for cluster_id in range(optimal_k_docs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"–ö–õ–ê–°–¢–ï–† {cluster_id}: {cluster_distribution[cluster_id]} –Ω–æ–≤–æ—Å—Ç–µ–π\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ\n",
        "    cluster_docs = df[df['cluster'] == cluster_id]\n",
        "\n",
        "    # –¶–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "    center = kmeans_docs.cluster_centers_[cluster_id]\n",
        "\n",
        "    # –¢–æ–ø-10 —Å–ª–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞ (–Ω–∞–∏–±–æ–ª—å—à–∏–µ –≤–µ—Å–∞)\n",
        "    top_indices = center.argsort()[-10:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_indices]\n",
        "\n",
        "    print(f\"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(top_words)}\")\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
        "    print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\")\n",
        "    for i, title in enumerate(cluster_docs['title'].head(3), 1):\n",
        "        print(f\"  {i}. {title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sqp_hjW3ZNa"
      },
      "source": [
        "## 7. –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –Ω–æ–≤–æ—Å—Ç—è–º\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Å–ª–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsG8Q_YE3ZNa"
      },
      "outputs": [],
      "source": [
        "def assign_tags_to_news(news_tokens, word_cluster_df, tags_dict, top_n=3):\n",
        "    \"\"\"\n",
        "    –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç —Ç–µ–≥–∏ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–ª–æ–≤.\n",
        "\n",
        "    Args:\n",
        "        news_tokens: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–æ–≤–æ—Å—Ç–∏\n",
        "        word_cluster_df: DataFrame —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ —Å–ª–æ–≤\n",
        "        tags_dict: —Å–ª–æ–≤–∞—Ä—å {cluster_id: [tags]}\n",
        "        top_n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ –¥–ª—è –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è\n",
        "\n",
        "    Returns:\n",
        "        —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤\n",
        "    \"\"\"\n",
        "    # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Å–ª–æ–≤–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "    cluster_counts = Counter()\n",
        "\n",
        "    for token in news_tokens:\n",
        "        word_info = word_cluster_df[word_cluster_df['word'] == token]\n",
        "        if not word_info.empty:\n",
        "            cluster_id = word_info['cluster'].iloc[0]\n",
        "            cluster_counts[cluster_id] += 1\n",
        "\n",
        "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-N –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "    top_clusters = cluster_counts.most_common(top_n)\n",
        "\n",
        "    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–≥–∏\n",
        "    assigned_tags = []\n",
        "    for cluster_id, _ in top_clusters:\n",
        "        if cluster_id in tags_dict:\n",
        "            assigned_tags.extend(tags_dict[cluster_id][:2])  # –±–µ—Ä–µ–º 2 —Ç–µ–≥–∞ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "\n",
        "    return assigned_tags[:top_n*2]  # –º–∞–∫—Å–∏–º—É–º top_n*2 —Ç–µ–≥–æ–≤\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏\n",
        "df['tags'] = df['all_tokens'].apply(\n",
        "    lambda x: assign_tags_to_news(x, word_cluster_df, tags_dict, top_n=3)\n",
        ")\n",
        "\n",
        "print(\"‚úì –¢–µ–≥–∏ –ø—Ä–∏—Å–≤–æ–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç—è–º\")\n",
        "print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ç–µ–≥–∞–º–∏:\\n\")\n",
        "for i in range(min(5, len(df))):\n",
        "    print(f\"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {df['title'].iloc[i][:80]}...\")\n",
        "    print(f\"–¢–µ–≥–∏: {', '.join(df['tags'].iloc[i])}\")\n",
        "    print(f\"–ö–ª–∞—Å—Ç–µ—Ä: {df['cluster'].iloc[i]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CW3VVP83ZNa"
      },
      "outputs": [],
      "source": [
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–≥–∞–º\n",
        "all_tags = []\n",
        "for tags in df['tags']:\n",
        "    all_tags.extend(tags)\n",
        "\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ï–ì–ê–ú\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {len(tag_counts)}\")\n",
        "print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {len(all_tags) / len(df):.1f}\")\n",
        "\n",
        "print(f\"\\n–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–µ–≥–æ–≤:\")\n",
        "for tag, count in tag_counts.most_common(20):\n",
        "    print(f\"  {tag}: {count} —Ä–∞–∑\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-—Ç–µ–≥–æ–≤\n",
        "top_tags = dict(tag_counts.most_common(15))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(list(top_tags.keys()), list(top_tags.values()), color='steelblue')\n",
        "plt.xlabel('–ß–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')\n",
        "plt.title('–¢–æ–ø-15 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–µ–≥–æ–≤', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72BPQWHK3ZNb"
      },
      "source": [
        "## 8. –ú–æ–¥–µ–ª—å 1 (Baseline): –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
        "\n",
        "–û–±—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXAYM80k3ZNb"
      },
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è supervised learning\n",
        "X = doc_vectors  # TF-IDF –≤–µ–∫—Ç–æ—Ä—ã\n",
        "y = df['cluster']  # –ö–ª–∞—Å—Ç–µ—Ä—ã –∫–∞–∫ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
        "\n",
        "# –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}\")\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:\")\n",
        "print(y_train.value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xoq2C2z13ZNb"
      },
      "outputs": [],
      "source": [
        "# Baseline –º–æ–¥–µ–ª—å: –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
        "print(\"–û–±—É—á–µ–Ω–∏–µ Baseline –º–æ–¥–µ–ª–∏ (Logistic Regression)...\")\n",
        "\n",
        "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "f1_baseline = f1_score(y_test, y_pred_baseline, average='weighted')\n",
        "precision_baseline = precision_score(y_test, y_pred_baseline, average='weighted')\n",
        "recall_baseline = recall_score(y_test, y_pred_baseline, average='weighted')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ BASELINE –ú–û–î–ï–õ–ò (Logistic Regression)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy_baseline:.4f}\")\n",
        "print(f\"Precision: {precision_baseline:.4f}\")\n",
        "print(f\"Recall:    {recall_baseline:.4f}\")\n",
        "print(f\"F1-score:  {f1_baseline:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_baseline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj_etKqt3ZNb"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix –¥–ª—è baseline\n",
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
        "           xticklabels=range(optimal_k_docs),\n",
        "           yticklabels=range(optimal_k_docs))\n",
        "plt.title('Confusion Matrix - Baseline (Logistic Regression)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GNiEXwn3ZNb"
      },
      "source": [
        "## 9. –ú–æ–¥–µ–ª—å 2 (–£–ª—É—á—à–µ–Ω–Ω–∞—è): Random Forest —Å GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt2xZmHd3ZNb"
      },
      "outputs": [],
      "source": [
        "# Random Forest —Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "print(\"–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (Random Forest —Å GridSearchCV)...\")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\n‚úì –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {grid_search.best_params_}\")\n",
        "print(f\"–õ—É—á—à–∏–π CV F1-score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL-wb4Gg3ZNc"
      },
      "outputs": [],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò (Random Forest)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy_rf:.4f}\")\n",
        "print(f\"Precision: {precision_rf:.4f}\")\n",
        "print(f\"Recall:    {recall_rf:.4f}\")\n",
        "print(f\"F1-score:  {f1_rf:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxZv6f_J3ZNc"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix –¥–ª—è Random Forest\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens',\n",
        "           xticklabels=range(optimal_k_docs),\n",
        "           yticklabels=range(optimal_k_docs))\n",
        "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDe3oFUI3ZNc"
      },
      "source": [
        "## 10. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kJTA_993ZNc"
      },
      "outputs": [],
      "source": [
        "# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n",
        "results_comparison = pd.DataFrame({\n",
        "    '–ú–æ–¥–µ–ª—å': ['Baseline (Logistic Regression)', 'Random Forest (GridSearchCV)'],\n",
        "    'Accuracy': [accuracy_baseline, accuracy_rf],\n",
        "    'Precision': [precision_baseline, precision_rf],\n",
        "    'Recall': [recall_baseline, recall_rf],\n",
        "    'F1-score': [f1_baseline, f1_rf],\n",
        "    'CV Score': ['-', grid_search.best_score_]\n",
        "})\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\")\n",
        "print(\"=\" * 80)\n",
        "print(results_comparison.to_string(index=False))\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "baseline_scores = [accuracy_baseline, precision_baseline, recall_baseline, f1_baseline]\n",
        "rf_scores = [accuracy_rf, precision_rf, recall_rf, f1_rf]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bars1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline (Logistic Reg.)', color='skyblue')\n",
        "bars2 = ax.bar(x + width/2, rf_scores, width, label='Random Forest', color='lightgreen')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "ax.set_ylim([0.7, 1.0])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                   xytext=(0, 3),\n",
        "                   textcoords=\"offset points\",\n",
        "                   ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "improvement = (f1_rf - f1_baseline) / f1_baseline * 100\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 6: Random Forest –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ F1-score –Ω–∞ {improvement:.1f}% \"\n",
        "      f\"–ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å baseline –º–æ–¥–µ–ª—å—é.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6gTgcur3ZNc"
      },
      "source": [
        "## 11. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsC-ginL3ZNc"
      },
      "outputs": [],
      "source": [
        "# –ù–∞—Ö–æ–¥–∏–º –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã\n",
        "test_indices = X_test.indices if hasattr(X_test, 'indices') else range(len(y_test))\n",
        "df_test = df.iloc[list(X_test.indices) if hasattr(X_test, 'indices') else y_test.index].copy()\n",
        "df_test['predicted'] = y_pred_rf\n",
        "df_test['actual'] = y_test.values\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –æ—à–∏–±–∫–∏\n",
        "errors = df_test[df_test['predicted'] != df_test['actual']].copy()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)}\")\n",
        "print(f\"–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫: {len(errors) / len(df_test) * 100:.2f}%\")\n",
        "\n",
        "print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π:\\n\")\n",
        "for i, (idx, row) in enumerate(errors.head(5).iterrows(), 1):\n",
        "    print(f\"\\n--- –ü—Ä–∏–º–µ—Ä {i} ---\")\n",
        "    print(f\"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {row['title'][:100]}...\")\n",
        "    print(f\"–†–µ–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä: {row['actual']}\")\n",
        "    print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä: {row['predicted']}\")\n",
        "    print(f\"–¢–µ–≥–∏: {', '.join(row['tags'][:5])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxAgRgQn3ZNd"
      },
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –ø—É—Ç–∞—é—â–∏—Ö—Å—è –ø–∞—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "error_pairs = Counter()\n",
        "for _, row in errors.iterrows():\n",
        "    pair = (row['actual'], row['predicted'])\n",
        "    error_pairs[pair] += 1\n",
        "\n",
        "print(\"\\n–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –ø—É—Ç–∞—é—â–∏–µ—Å—è –ø–∞—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:\")\n",
        "for (actual, predicted), count in error_pairs.most_common(5):\n",
        "    print(f\"  –ö–ª–∞—Å—Ç–µ—Ä {actual} ‚Üí –ö–ª–∞—Å—Ç–µ—Ä {predicted}: {count} –æ—à–∏–±–æ–∫\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIou6g-F3ZNd"
      },
      "source": [
        "## 12. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxKbAvwh3ZNd"
      },
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è Random Forest\n",
        "feature_importance = best_rf.feature_importances_\n",
        "feature_names = doc_vectorizer.get_feature_names_out()\n",
        "\n",
        "# –¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "indices = np.argsort(feature_importance)[-20:]\n",
        "top_features = [(feature_names[i], feature_importance[i]) for i in indices]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"–¢–û–ü-20 –°–ê–ú–´–• –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (—Å–ª–æ–≤)\")\n",
        "print(\"=\" * 60)\n",
        "for feature, importance in reversed(top_features):\n",
        "    print(f\"  {feature}: {importance:.4f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh([f[0] for f in top_features], [f[1] for f in top_features], color='coral')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('–¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä –í—ã–≤–æ–¥ 7: –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –≤—ã—è–≤–∏–ª–∞ –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞, \"\n",
        "      f\"–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å —Ç–µ–º–∞—Ç–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmYt80xP3ZNd"
      },
      "source": [
        "## 13. –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY403_jB3ZNd"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´ –ü–†–û–ï–ö–¢–ê\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "1. –î–ê–ù–ù–´–ï:\n",
        "   - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "   - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏: {df['text_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\n",
        "   - –°–ª–æ–≤–∞—Ä—å: {len(word_counts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤\n",
        "\n",
        "2. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –°–õ–û–í:\n",
        "   - –°–æ–∑–¥–∞–Ω–æ {optimal_k_words} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–ª–æ–≤ (—Ç–µ–≥–æ–≤)\n",
        "   - Silhouette Score: {sil_score_words:.3f}\n",
        "   - Davies-Bouldin Index: {db_score_words:.3f}\n",
        "\n",
        "3. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –ù–û–í–û–°–¢–ï–ô:\n",
        "   - –ù–æ–≤–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ {optimal_k_docs} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "   - Silhouette Score: {sil_score_docs:.3f}\n",
        "   - Davies-Bouldin Index: {db_score_docs:.3f}\n",
        "\n",
        "4. –ú–û–î–ï–õ–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:\n",
        "\n",
        "   Baseline (Logistic Regression):\n",
        "   - Accuracy: {accuracy_baseline:.4f}\n",
        "   - F1-score: {f1_baseline:.4f}\n",
        "\n",
        "   –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (Random Forest):\n",
        "   - Accuracy: {accuracy_rf:.4f}\n",
        "   - F1-score: {f1_rf:.4f}\n",
        "   - –£–ª—É—á—à–µ–Ω–∏–µ: +{(f1_rf - f1_baseline)*100:.2f}%\n",
        "\n",
        "5. –ö–õ–Æ–ß–ï–í–´–ï –î–û–°–¢–ò–ñ–ï–ù–ò–Ø:\n",
        "   - ‚úì –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ\n",
        "   - ‚úì Random Forest –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ({accuracy_rf:.1%})\n",
        "   - ‚úì –°–∏—Å—Ç–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n",
        "   - ‚úì –í—ã—è–≤–ª–µ–Ω—ã –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã\n",
        "\n",
        "6. –ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–ú–ï–ù–ï–ù–ò–ï:\n",
        "   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞\n",
        "   - –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π\n",
        "   - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç–µ–º–∞—Ç–∏–∫ –≤ –°–ú–ò\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuEAhKS-3ZNe"
      },
      "source": [
        "## 14. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-XHbF1Y3ZNe"
      },
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "import pickle\n",
        "\n",
        "# –ú–æ–¥–µ–ª–∏\n",
        "with open('/mnt/user-data/outputs/best_rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_rf, f)\n",
        "\n",
        "with open('/mnt/user-data/outputs/baseline_model.pkl', 'wb') as f:\n",
        "    pickle.dump(baseline_model, f)\n",
        "\n",
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä\n",
        "with open('/mnt/user-data/outputs/vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(doc_vectorizer, f)\n",
        "\n",
        "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "df[['title', 'cluster', 'tags']].to_csv('/mnt/user-data/outputs/news_with_clusters_and_tags.csv',\n",
        "                                         index=False, encoding='utf-8')\n",
        "\n",
        "word_cluster_df.to_csv('/mnt/user-data/outputs/word_clusters.csv', index=False, encoding='utf-8')\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–≥–∏\n",
        "with open('/mnt/user-data/outputs/tags_dictionary.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump({str(k): v for k, v in tags_dict.items()}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
        "print(\"  - best_rf_model.pkl\")\n",
        "print(\"  - baseline_model.pkl\")\n",
        "print(\"  - vectorizer.pkl\")\n",
        "print(\"  - news_with_clusters_and_tags.csv\")\n",
        "print(\"  - word_clusters.csv\")\n",
        "print(\"  - tags_dictionary.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}